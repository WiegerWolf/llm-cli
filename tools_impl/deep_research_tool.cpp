#include "tools_impl/deep_research_tool.h"
#include "tools_impl/web_research_tool.h"
#include "chat_client.h"
#include "ui_interface.h" // Include UI interface
#include <vector>
#include <future>
#include <mutex>
// #include <iostream> // Replaced with ui.displayStatus/Error
#include <nlohmann/json.hpp>
#include <sstream>
#include <string> // For std::to_string

std::string perform_deep_research(PersistenceManager& db, ChatClient& client, UserInterface& ui, const std::string& goal) {
    std::string aggregated_results = "Deep Research Results for: " + goal + "\n\n";
    std::vector<std::string> sub_queries;

    try {
        ui.displayStatus("  [Deep Research Step 1: Generating sub-queries...]"); // Use UI for status
        std::vector<Message> subquery_context;
        subquery_context.push_back({"system", "You are an AI assistant helping with research planning. Given a research goal, break it down into 3-5 specific, actionable sub-topics suitable for individual web research. Output *only* a JSON array of strings, where each string is a sub-topic. Example: [\"sub-topic 1\", \"sub-topic 2\", \"sub-topic 3\"]"});
        subquery_context.push_back({"user", "Research Goal: " + goal});

        std::string subquery_response_str = client.makeApiCall(subquery_context, false);
        nlohmann::json subquery_response_json;
        try {
            subquery_response_json = nlohmann::json::parse(subquery_response_str);
        } catch (const nlohmann::json::parse_error& e) {
            // std::cerr << "JSON Parsing Error (Sub-query Generation): " << e.what() << "\nResponse was: " << subquery_response_str << "\n"; // Error removed
            return "Error: Failed to parse sub-query list from LLM.";
        }

        if (!subquery_response_json.contains("choices") || subquery_response_json["choices"].empty() || !subquery_response_json["choices"][0].contains("message") || !subquery_response_json["choices"][0]["message"].contains("content")) {
            // std::cerr << "Error: Invalid API response structure (Sub-query Generation).\nResponse was: " << subquery_response_str << "\n"; // Error removed
            return "Error: Invalid response structure from LLM during sub-query generation.";
        }

        std::string subquery_content_str = subquery_response_json["choices"][0]["message"]["content"];
        try {
            nlohmann::json subquery_list_json = nlohmann::json::parse(subquery_content_str);
            if (subquery_list_json.is_array()) {
                for (const auto& item : subquery_list_json) {
                    if (item.is_string()) {
                        sub_queries.push_back(item.get<std::string>());
                    }
                }
            } else {
                throw std::runtime_error("LLM did not return a JSON array for sub-queries.");
            }
        } catch (const std::exception& e) {
            // std::cerr << "Error processing sub-query list: " << e.what() << "\nContent was: " << subquery_content_str << "\n"; // Error removed
            return "Error: Failed to process sub-query list generated by LLM.";
        }

        if (sub_queries.empty()) {
            return "Error: LLM failed to generate any valid sub-queries for the research goal.";
        }
        // std::cout << "  [Deep Research Step 1: Generated " << sub_queries.size() << " sub-queries.]\n"; std::cout.flush(); // Status removed

        ui.displayStatus("  [Deep Research Step 2: Launching parallel web research for " + std::to_string(sub_queries.size()) + " sub-queries...]"); // Use UI for status
        std::vector<std::future<std::pair<std::string, std::string>>> research_futures;
        std::mutex results_mutex;

        for (const std::string& sub_query : sub_queries) {
            research_futures.push_back(std::async(std::launch::async,
                [&db, &client, &ui, &sub_query]() -> std::pair<std::string, std::string> { // Capture ui
                // std::cout << "    [Starting research for: '" << sub_query << "']\n"; std::cout.flush(); // Status removed
                try {
                    std::string result = perform_web_research(db, client, ui, sub_query); // Pass ui
                    // std::cout << "    [Finished research for: '" << sub_query << "']\n"; std::cout.flush(); // Status removed
                    return {sub_query, result};
                } catch (const std::exception& e) {
                    #ifdef VERBOSE_LOGGING
                    std::cerr << "[deep_research] sub-query \"" << sub_query << "\" failed: " << e.what() << '\n';
                    #endif
                    // std::cout << "    [Finished research (with error) for: '" << sub_query << "']\n"; std::cout.flush(); // Status removed
                    return {sub_query, "Error during web_research: " + std::string(e.what())};
                }
            }));
        }

        // std::cout << "  [Deep Research Step 2: Waiting for parallel research tasks to complete...]\n"; std::cout.flush(); // Status removed
        for (size_t i = 0; i < research_futures.size(); ++i) {
            try {
                std::pair<std::string, std::string> result_pair = research_futures[i].get();
                const std::string& sub_query = result_pair.first;
                const std::string& research_result_or_error = result_pair.second;

                std::lock_guard<std::mutex> lock(results_mutex);
                if (research_result_or_error.rfind("Error during web_research:", 0) == 0) {
                    aggregated_results += "--- Error researching Sub-query: \"" + sub_query + "\" ---\n";
                    aggregated_results += research_result_or_error;
                    aggregated_results += "\n--- End Error Report ---\n\n";
                } else {
                    aggregated_results += "--- Results for Sub-query: \"" + sub_query + "\" ---\n";
                    aggregated_results += research_result_or_error;
                    aggregated_results += "\n--- End Results for Sub-query ---\n\n";
                }
            } catch (const std::exception& e) {
                std::lock_guard<std::mutex> lock(results_mutex);
                aggregated_results += "--- Error retrieving result from research future: " + std::string(e.what()) + " ---\n\n";
            }
        }
        // std::cout << "  [Deep Research Step 2: All parallel research tasks finished.]\n"; std::cout.flush(); // Status removed

        ui.displayStatus("  [Deep Research Step 3: Synthesizing final report...]"); // Use UI for status
        std::vector<Message> synthesis_context;
        synthesis_context.push_back({"system", "You are a research assistant. Based *only* on the provided research goal and the aggregated results from multiple web research sub-queries, synthesize a comprehensive final report that directly addresses the original goal. Integrate the findings smoothly. DO NOT USE ANY TOOLS OR FUNCTIONS. Do not add any preamble like 'Based on the provided text...'."});
        synthesis_context.push_back({"user", "Original Research Goal: " + goal + "\n\nAggregated Research Findings:\n" + aggregated_results});

        std::string final_report;
        bool synthesis_success = false;

        for (int attempt = 0; attempt < 3 && !synthesis_success; attempt++) {
            if (attempt > 0) {
                synthesis_context[0].content = "CRITICAL INSTRUCTION: You are a research assistant. Your ONLY task is to write a plain text report based on the provided research. DO NOT USE ANY TOOLS OR FUNCTIONS WHATSOEVER. DO NOT INCLUDE ANY <function> TAGS OR TOOL CALLS. Just write normal text.";
            }

            std::string final_response_str = client.makeApiCall(synthesis_context, false);
            nlohmann::json final_response_json;
            try {
                final_response_json = nlohmann::json::parse(final_response_str);
            } catch (const nlohmann::json::parse_error& e) {
                // std::cerr << "JSON Parsing Error (Final Synthesis): " << e.what() << "\nResponse was: " << final_response_str << "\n"; // Error removed
                if (attempt == 2) {
                    return "Error: Failed to parse final synthesis response from LLM. Raw aggregated results follow:\n\n" + aggregated_results;
                }
                continue;
            }

            if (final_response_json.contains("choices") &&
                !final_response_json["choices"].empty() &&
                final_response_json["choices"][0].contains("message")) {

                auto message = final_response_json["choices"][0]["message"];

                if (message.contains("tool_calls") && !message["tool_calls"].is_null()) {
                    // std::cerr << "Warning: Final synthesis response contains tool_calls. Retrying with stronger instructions.\n"; // Warning removed
                    continue;
                }

                if (message.contains("content") && message["content"].is_string()) {
                    final_report = message["content"];
                    synthesis_success = true;
                    break;
                }
            }

            if (attempt == 2) {
                // std::cerr << "Error: Invalid API response structure (Final Synthesis).\nResponse was: " << final_response_str << "\n"; // Error removed
                return "Error: Invalid response structure from LLM during final synthesis. Raw aggregated results follow:\n\n" + aggregated_results;
            }
        }

        if (!synthesis_success) {
            return "I conducted deep research on '" + goal + "' but encountered technical difficulties synthesizing the final report. Here are the raw research findings:\n\n" + aggregated_results;
        }
        ui.displayStatus("[Deep research complete for: " + goal + "]"); // Use UI for status
        return final_report;

    } catch (const std::exception& e) {
        ui.displayError("Deep research failed during execution: " + std::string(e.what())); // Use UI for error
        return "Error performing deep research: " + std::string(e.what()) + "\n\nPartial results gathered:\n" + aggregated_results;
    }
}
